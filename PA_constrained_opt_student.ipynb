{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Secret data generating mechanism that as the optimisation engineer you don't have access to. \n",
    "# However, to evaluate the final solution beyond the training objective \n",
    "# (which is quite likely close to zero for any training procedure due to huge number of parameters) \n",
    "# we need access to w_star. \n",
    "\n",
    "# You may feel free to use the variable data_dim. All other variables here are off limits. \n",
    "\n",
    "# Data description: 30 data points for 100 dimensional regression problem. \n",
    "\n",
    "data_dim = 100\n",
    "num_data = 30\n",
    "sparsity = 3\n",
    "np.random.seed(100) \n",
    "w_temp = np.random.randn(sparsity)\n",
    "indices = np.random.choice(data_dim, sparsity, replace=False)\n",
    "w_star=np.zeros(data_dim)\n",
    "w_star[indices]=w_temp\n",
    "\n",
    "data_matrix = np.random.randn(num_data, data_dim)*0.5\n",
    "labels = np.dot(data_matrix, w_star) + 0.0*np.random.randn(num_data)\n",
    "\n",
    "\n",
    "\n",
    "global_A = np.dot(data_matrix.T, data_matrix) / num_data\n",
    "global_b = np.dot(data_matrix.T,labels) / num_data\n",
    "global_c = 0.5* np.dot(labels, labels) / num_data\n",
    "global_w_star = np.array(w_star)\n",
    "\n",
    "print('w_star=', global_w_star)\n",
    "print('Non-zero coeffs=', np.where(np.abs(global_w_star)>1e-5))\n",
    "eig_vals, eig_vecs = np.linalg.eig(global_A)\n",
    "smoothness_coeff = np.real(np.max(eig_vals))\n",
    "print('Smoothness coeff=', smoothness_coeff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_value(w):\n",
    "    A = global_A\n",
    "    b = global_b\n",
    "    return 0.5*np.dot(w, np.dot(A,w)) - np.dot(b,w) + global_c\n",
    "\n",
    "def get_gradient(w):\n",
    "    A = global_A\n",
    "    b = global_b\n",
    "    return np.dot(A,w)-b\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "\n",
    "Assume you have oracle access to a regression error function (call it f) on some training data. Solve the LASSO optimisation problem, over some given l1 norm ball using the projected gradient descent.\n",
    "\n",
    "1. Write the function for the l1 norm projection oracle, l2 norm projection oracle and the respective PGD steps.\n",
    "\n",
    "\n",
    "2. Plot f(w_t) vs t for the PGD algorithm with l2 norm projection oracle. The step size is fixed to eta=0.01. Repeat the plots with different l2 norm constraints: 0.1, 0.5, 1, 2, 5 (on the same plot). \n",
    "\n",
    "3. Plot f(w_t) vs t for the PGD algorithm with l1 norm projection oracle. The step size is fixed to eta=0.01. Repeat the plots with different l1 norm constraints: 0.1, 0.5, 1, 2, 5 (on the same plot).\n",
    "\n",
    "4. Let $w^1_\\lambda$ be the result of running PGD with the l1 norm constraint of lambda for 500 iterations. Similarly let $w^2_\\mu$ be the result of running PGD with the l2 norm constraint of mu for 500 iterations. Give two separate cells with each cell containing 2 plots as follows: \n",
    "\n",
    "    4a. In the first cell: $f(w^1_\\lambda)$ vs $\\lambda$ and $||w^1_\\lambda - w^* ||$ vs $\\lambda$.    \n",
    "    4b. In the Second cell: $f(w^2_\\mu)$ vs $\\mu$ and $||w^2_\\mu - w^* ||$ vs $\\mu$. \n",
    "    \n",
    "    Note 1: $w^*$ is the vector used in data generation. \n",
    "    Note 2: Using $w^*$ is illegal anywhere else. But it's okay here to just make a point.\n",
    "    Note 3: The range of mu and lambda to plot may be taken to be 0.1 to 3.\n",
    "    Note 4: We can take the norm above to simply be the difference in the set of non-zero components\n",
    "\n",
    "5. Summarise and explain your findings in a markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 1\n",
    "\n",
    "def projection_oracle_l2(w, l2_norm):\n",
    "    pass\n",
    "\n",
    "def projection_oracle_l1(w, l1_norm):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "def PGD_step_l2(w, l2_norm, eta=0.1):\n",
    "    pass\n",
    "\n",
    "def PGD_step_l1(w, l1_norm, eta=0.1):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 2\n",
    "\n",
    "f_vals_all=[]\n",
    "plt.figure()\n",
    "norm_constraint_vals = [0.1, 0.5, 1, 2, 5]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    f_vals=[]\n",
    "    w=np.zeros(data_dim)\n",
    "\n",
    "    for t in range(100):\n",
    "        w = PGD_step_l2(w, norm_constraint)\n",
    "        f_vals.append(get_value(w))\n",
    "    f_vals_all.append(f_vals)\n",
    "plt.plot(range(100),np.array(f_vals_all).T)\n",
    "plt.legend(norm_constraint_vals)\n",
    "plt.xlabel('Iteration: t')\n",
    "plt.ylabel('Loss : $f(x_t)$')\n",
    "plt.title('Loss vs Iteration for l2-norm ball PGD, with different l2 norm constraints')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 3\n",
    "\n",
    "f_vals_all=[]\n",
    "plt.figure()\n",
    "norm_constraint_vals = [0.1, 0.5, 1, 2, 5]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    f_vals=[]\n",
    "    w=np.zeros(data_dim)\n",
    "    for t in range(100):\n",
    "        w = PGD_step_l1(w, norm_constraint)\n",
    "        f_vals.append(get_value(w))\n",
    "    f_vals_all.append(f_vals)\n",
    "plt.plot(range(100),np.array(f_vals_all).T)\n",
    "plt.legend(norm_constraint_vals)\n",
    "plt.xlabel('Iteration: t')\n",
    "plt.ylabel('Loss : $f(x_t)$')\n",
    "plt.title('Loss vs Iteration for l1-norm ball PGD, with different l1 norm constraints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 4a\n",
    "\n",
    "norm_constraint_vals = np.arange(0.1,3,0.1)\n",
    "loss_values=[]\n",
    "deviation_from_truth=[]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    w=np.zeros(data_dim)\n",
    "    for t in range(100):\n",
    "        w = PGD_step_l2(w, norm_constraint)\n",
    "    loss_values.append(get_value(w))\n",
    "\n",
    "    # One measure of deviation is to check the non-zero components that are \n",
    "    # different in w and w_star\n",
    "    \n",
    "    nonzero_w_star = set(np.where(np.abs(global_w_star)>1e-5)[0])\n",
    "    nonzero_w = set(np.where(np.abs(w)>1e-5)[0])\n",
    "    deviation_from_truth.append( len(nonzero_w.symmetric_difference(nonzero_w_star)) )\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, loss_values)\n",
    "plt.xlabel('l2 norm constraint')\n",
    "plt.ylabel('Loss value at convergence')\n",
    "plt.title('L2 norm projected gradient descent')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, deviation_from_truth)\n",
    "plt.xlabel('l2 norm constraint')\n",
    "plt.ylabel('Deviation of learned $w$ from $w^*$')\n",
    "plt.title('L2 norm projected gradient descent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 4b\n",
    "\n",
    "norm_constraint_vals = np.arange(0.1,3,0.1)\n",
    "loss_values=[]\n",
    "deviation_from_truth=[]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    w=np.zeros(data_dim)\n",
    "    for t in range(100):\n",
    "        w = PGD_step_l1(w, norm_constraint)\n",
    "    loss_values.append(get_value(w))\n",
    "    \n",
    "    # One measure of deviation is to check the non-zero components that are \n",
    "    # different in w and w_star\n",
    "    \n",
    "    nonzero_w_star = set(np.where(np.abs(global_w_star)>1e-5)[0])\n",
    "    nonzero_w = set(np.where(np.abs(w)>1e-5)[0])\n",
    "    deviation_from_truth.append( len(nonzero_w.symmetric_difference(nonzero_w_star)) )\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, loss_values)\n",
    "plt.xlabel('l1 norm constraint')\n",
    "plt.ylabel('Loss value at convergence')\n",
    "plt.title('L1 norm projected gradient descent')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, deviation_from_truth)\n",
    "plt.xlabel('l1 norm constraint')\n",
    "plt.ylabel('Deviation of learned $w$ from $w^*$')\n",
    "plt.title('L1 norm projected gradient descent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 Part 5:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "\n",
    "Assume you have oracle access to a regression error function (call it f) on some training data. Solve the LASSO optimisation problem, over some given l1 norm ball using the Frank-Wolfe method. \n",
    "\n",
    "1. Write the function for the l1 norm LMO, l2 norm LMO and the respective FW steps.\n",
    "\n",
    "2. Plot f(w_t) vs t for the FW algorithm with l2 norm LMO. Repeat the plots with different l2 norm constraints: 0.1,0.5,1,2,5 (on the same plot). \n",
    "\n",
    "3. Plot f(w_t) vs t for the FW algorithm with l1 norm LMO. Repeat the plots with different l1 norm constraints: 0.1,0.5,1,2,5 (on the same plot). \n",
    "\n",
    "4. Let $w^1_\\lambda$ be the result of running FW with the l1 norm constraint of lambda for 500 iterations. Similarly let $w^2_\\mu$ be the result of running FW with the l2 norm constraint of mu for 500 iterations. Give two separate plots with each plot containing 2 curves as follows: \n",
    "\n",
    "    4a. In the first plot: $f(w^1_\\lambda)$ vs $\\lambda$ and $||w^1_\\lambda - w^* ||$ vs $\\lambda$.    \n",
    "    4b. In the Second plot: $f(w^2_\\mu)$ vs $\\mu$ and $||w^2_\\mu - w^* ||$ vs $\\mu$. \n",
    "    \n",
    "    Note 1: $w^*$ is the vector used in data generation. \n",
    "    Note 2: Using $w^*$ is illegal anywhere else. But it's okay here to just make a point.\n",
    "    Note 3: The range of mu and lambda to plot may be taken to be 0.1 to 3.\n",
    "    Note 4: We can take the norm above to simply be the difference in the set of non-zero components\n",
    "    \n",
    "5. Summarise and explain your findings on 2,3,4 above in a markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2 part 1\n",
    "\n",
    "\n",
    "def LMO_l2(u, l2_norm):\n",
    "    pass\n",
    "\n",
    "\n",
    "def LMO_l1(u, l1_norm):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "def FW_step_l2(w, l2_norm):\n",
    "    pass    \n",
    "\n",
    "def FW_step_l1(w, l1_norm):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2 part 2\n",
    "\n",
    "f_vals_all=[]\n",
    "plt.figure()\n",
    "norm_constraint_vals = [0.1, 0.5, 1, 2, 5]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    f_vals=[]\n",
    "    w=np.zeros(data_dim)\n",
    "\n",
    "    for t in range(100):\n",
    "        w = FW_step_l2(w, norm_constraint)\n",
    "        f_vals.append(get_value(w))\n",
    "    f_vals_all.append(f_vals)\n",
    "plt.plot(range(100),np.array(f_vals_all).T)\n",
    "plt.legend(norm_constraint_vals)\n",
    "plt.xlabel('Iteration: t')\n",
    "plt.ylabel('Loss : $f(x_t)$')\n",
    "plt.title('Loss vs Iteration for l2-norm ball PGD, with different l2 norm constraints')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2 part 3\n",
    "\n",
    "f_vals_all=[]\n",
    "plt.figure()\n",
    "norm_constraint_vals = [0.1, 0.5, 1, 2, 5]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    f_vals=[]\n",
    "    w=np.zeros(data_dim)\n",
    "    for t in range(100):\n",
    "        w = FW_step_l1(w, norm_constraint)\n",
    "        f_vals.append(get_value(w))\n",
    "    f_vals_all.append(f_vals)\n",
    "plt.plot(range(100),np.array(f_vals_all).T)\n",
    "plt.legend(norm_constraint_vals)\n",
    "plt.xlabel('Iteration: t')\n",
    "plt.ylabel('Loss : $f(x_t)$')\n",
    "plt.title('Loss vs Iteration for l1-norm ball PGD, with different l1 norm constraints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2 part 4a\n",
    "\n",
    "\n",
    "norm_constraint_vals = np.arange(0.1,3,0.1)\n",
    "loss_values=[]\n",
    "deviation_from_truth=[]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    w=np.zeros(data_dim)\n",
    "    for t in range(100):\n",
    "        w = FW_step_l2(w, norm_constraint)\n",
    "    loss_values.append(get_value(w))\n",
    "\n",
    "    # One measure of deviation is to check the non-zero components that are \n",
    "    # different in w and w_star\n",
    "    \n",
    "    nonzero_w_star = set(np.where(np.abs(global_w_star)>1e-5)[0])\n",
    "    nonzero_w = set(np.where(np.abs(w)>1e-5)[0])\n",
    "    deviation_from_truth.append( len(nonzero_w.symmetric_difference(nonzero_w_star)) )\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, loss_values)\n",
    "plt.xlabel('l2 norm constraint')\n",
    "plt.ylabel('Loss value at convergence')\n",
    "plt.title('L2 norm projected gradient descent')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, deviation_from_truth)\n",
    "plt.xlabel('l2 norm constraint')\n",
    "plt.ylabel('Deviation of learned $w$ from $w^*$')\n",
    "plt.title('L2 norm projected gradient descent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2 part 4b\n",
    "\n",
    "\n",
    "norm_constraint_vals = np.arange(0.1,3,0.1)\n",
    "loss_values=[]\n",
    "deviation_from_truth=[]\n",
    "for norm_constraint in norm_constraint_vals:\n",
    "    w=np.zeros(data_dim)\n",
    "    for t in range(100):\n",
    "        w = FW_step_l1(w, norm_constraint)\n",
    "    loss_values.append(get_value(w))\n",
    "    \n",
    "    # One measure of deviation is to check the non-zero components that are \n",
    "    # different in w and w_star\n",
    "    \n",
    "    nonzero_w_star = set(np.where(np.abs(global_w_star)>1e-5)[0])\n",
    "    nonzero_w = set(np.where(np.abs(w)>1e-5)[0])\n",
    "    deviation_from_truth.append( len(nonzero_w.symmetric_difference(nonzero_w_star)) )\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, loss_values)\n",
    "plt.xlabel('l1 norm constraint')\n",
    "plt.ylabel('Loss value at convergence')\n",
    "plt.title('L1 norm projected gradient descent')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(norm_constraint_vals, deviation_from_truth)\n",
    "plt.xlabel('l1 norm constraint')\n",
    "plt.ylabel('Deviation of learned $w$ from $w^*$')\n",
    "plt.title('L1 norm projected gradient descent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 part 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
