{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Test function 1: Convex 2d- Quadratic\n",
    "n_dim = 2\n",
    "def get_value(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    f(x) : a scalar\n",
    "    \"\"\"\n",
    "    A=np.array([[5.,4],[4.,5.]])\n",
    "    b=np.array([7.,2.])\n",
    "    return 0.5*np.dot(x, np.dot(A,x)) - np.dot(b,x)\n",
    "\n",
    "def get_gradient(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    grad f(x) : numpy array with same 1-dimensional shape as input x\n",
    "    \"\"\"\n",
    "    A=np.array([[5.,4],[4.,5.]])\n",
    "    b=np.array([7.,2.])\n",
    "    return np.dot(A,x) - b\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "\n",
    "Assume you are given access to the value and gradient oracles. (Two examples for testing given above called f1 and f2). You are allowed to access the function only through these oracles. \n",
    "\n",
    "1. Complete the code for gradient descent with different step size schemes. (one of them given for free)\n",
    "\n",
    "2. On f1, plot the the error vs iteration and trajectory plots for constant step sizes with the following values of eta: [0.1, 0.2, 0.23]. (20 Iterations should be sufficient)\n",
    "\n",
    "3. On f1, Plot the error vs iteration and trajectory plots for AG and FR rules. (20 Iterations should be sufficient)\n",
    "\n",
    "4. On f2, plot the the error vs iteration and trajectory plots for constant step size eta=1.5 with three different random initialisations drawn from N(0,0.01).  (20 Iterations should be sufficient)\n",
    "\n",
    "5. On f2, Plot the error vs iteration and trajectory plots for AG and FR rules for one random initialisation from N(0,0.01). (20 Iterations should be sufficient)\n",
    "\n",
    "6. Create a Markdown cell and give your findings for questions 2 and 3 above in them in an itemised list. (For this you may look at the function definitions in the oracle to get an idea).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 1\n",
    "\n",
    "def gradient_descent_update(x, eta=1.):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input. Representing x_t\n",
    "    eta: step size\n",
    "    \n",
    "    Returns:\n",
    "    x_next: 1-dimensional numpy array with same shape as x. \n",
    "    Representing one Gradient descent step with constant step size eta.. \n",
    "    \n",
    "    Notes:\n",
    "    You are allowed ONLY ONE get_gradient call in this function.\n",
    "    \"\"\"\n",
    "    grad = get_gradient(x)\n",
    "    return x-eta*grad\n",
    "\n",
    "def gradient_descent_update_AG(x, c=0.25, c_dash=0.5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input. Representing x_t\n",
    "    c: optional parameter for specifying AG rules. (Upper bound line)\n",
    "    c_dash: optional parameter for specifying AG rules. (Lower bound line)\n",
    "    \n",
    "    Returns:\n",
    "    x_next: 1-dimensional numpy array with same shape as x. \n",
    "    Representing one Gradient descent step with step size given by AG conditions. \n",
    "    \n",
    "    Notes:\n",
    "    You are allowed ONLY ONE get_gradient calls in this function.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def gradient_descent_update_FR(x, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x : 1 dimensional numpy array as input. Representing x_t\n",
    "    tol : tolerance parameter.\n",
    "    \n",
    "    Returns:\n",
    "    x_next: 1-dimensional numpy array with same shape as x. \n",
    "    Representing next iterate of Gradient descent step with step size given by full relaxation. \n",
    "    You may stop when the line search optimisation problem is solved to a tolerance given by tol.\n",
    "    \n",
    "    Notes:\n",
    "    You are allowed a FEW get_gradient calls in this function. But be careful not to go overboard.\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot / Utility functions\n",
    "\n",
    "def plot_error_vs_iteration(f_vals, plt_title=None):\n",
    "    # Plot the loss vs iteration number\n",
    "    # Argument f_vals is (n)-shape numpy array\n",
    "    \n",
    "    plt.figure()\n",
    "\n",
    "    if plt_title is None:\n",
    "        plt.title('Function value vs Iteration')\n",
    "    else:\n",
    "        plt.title(plt_title)\n",
    "\n",
    "    plt.xlabel('Iteration : t ')\n",
    "    plt.ylabel('Value : $f(x_t)$')\n",
    "    plt.plot(f_vals)\n",
    "\n",
    "\n",
    "def plot_trajectory(x_list, x_axis=None, y_axis=None, plt_title=None):\n",
    "    # Trajectory plot for the special case of 2-d functions\n",
    "    # Generating contour maps of function\n",
    "    \n",
    "    # Argument x_list is n by 2 numpy array giving n iterates in 2d. \n",
    "    \n",
    "    delta=0.025\n",
    "    if x_axis is None:\n",
    "        x_axis = np.arange(np.min(x_list[:,0])-0.5, np.max(x_list[:,0])+0.5 , delta)\n",
    "    if y_axis is None:\n",
    "        y_axis = np.arange(np.min(x_list[:,1])-0.5, np.max(x_list[:,1])+0.5 , delta)\n",
    "\n",
    "    X, Y = np.meshgrid(x_axis, y_axis)\n",
    "\n",
    "    Z = np.zeros(X.shape)\n",
    "\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            input_vec = np.array([X[i,j],Y[i,j]])\n",
    "            Z[i,j] = get_value(input_vec) \n",
    "\n",
    "    levels = np.linspace(Z.min(), Z.max(), 50)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10,10.*Z.shape[0]/Z.shape[1]))\n",
    "    if plt_title is None:\n",
    "        plt.title('Gradient Descent iterate trajectory')\n",
    "    else:\n",
    "        plt.title(plt_title)\n",
    "        \n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.contourf(X, Y, Z, levels=levels, cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "    # Plot the trajectory\n",
    "    plt.plot(x_list[:,0],x_list[:,1], marker='o', markersize=10, lw=5)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 2:\n",
    "\n",
    "for eta in [0.1, 0.2, 0.23]:\n",
    "    f_vals=[]\n",
    "    x_list=[]\n",
    "    x=np.zeros((n_dim))+np.random.randn(n_dim)*0.1\n",
    "\n",
    "    f_vals = []\n",
    "    for t in range(20):\n",
    "\n",
    "        x_list.append(x)\n",
    "        f_vals.append(get_value(x))\n",
    "        x = gradient_descent_update(x=x, eta=eta)\n",
    "    x_list = np.array(x_list)\n",
    "\n",
    "\n",
    "    plot_error_vs_iteration(f_vals, plt_title='GD iterate loss for eta='+str(eta))\n",
    "\n",
    "    if n_dim==2:\n",
    "        plot_trajectory(x_list, plt_title='GD trajectory with eta='+str(eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 3:\n",
    "for stepsize_rule in ['AG', 'FR']:\n",
    "    f_vals=[]\n",
    "    x_list=[]\n",
    "    x=np.zeros((n_dim))+np.random.randn(n_dim)*0.1\n",
    "\n",
    "    f_vals = []\n",
    "    for t in range(20):\n",
    "\n",
    "        x_list.append(x)\n",
    "        f_vals.append(get_value(x))\n",
    "        if stepsize_rule=='AG':\n",
    "            x = gradient_descent_update_AG(x=x, eta=eta)\n",
    "        elif stepsize_rule=='FR':\n",
    "            x = gradient_descent_update_FR(x=x, eta=eta)\n",
    "            \n",
    "    x_list = np.array(x_list)\n",
    "\n",
    "\n",
    "    plot_error_vs_iteration(f_vals, plt_title='GD iterate loss for step size rule '+stepsize_rule)\n",
    "\n",
    "    if n_dim==2:\n",
    "        plot_trajectory(x_list, plt_title='GD trajectory with step size rule '+ stepsize_rule)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test function 2: Non-convex 2d- function with 2 local minima\n",
    "n_dim=2\n",
    "def get_value(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    f(x) : a scalar\n",
    "    \"\"\"\n",
    "    p1=np.array([0,0.5])\n",
    "    p2=np.array([-0.5,0])\n",
    "    g=np.array([0.01,0.01])\n",
    "    \n",
    "    return (np.linalg.norm(x-p1)**2)*(np.linalg.norm(x-p2)**2)+ np.dot(g,x) \n",
    "\n",
    "def get_gradient(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    grad f(x) : numpy array with same 1-dimensional shape as input x\n",
    "    \"\"\"\n",
    "    p1=np.array([0,0.5])\n",
    "    p2=np.array([-0.5,0])\n",
    "    g=np.array([0.01,0.01])\n",
    "\n",
    "    term1= 2*(np.linalg.norm(x-p1)**2)*(x-p2)\n",
    "    term2= 2*(np.linalg.norm(x-p2)**2)*(x-p1)    \n",
    "    \n",
    "    \n",
    "    return term1+term2+g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1, part 4\n",
    "\n",
    "for eta in [1.5, 1.5, 1.5]:\n",
    "    f_vals=[]\n",
    "    x_list=[]\n",
    "    x=np.random.randn(n_dim)*0.1\n",
    "\n",
    "    f_vals = []\n",
    "    for t in range(20):\n",
    "        x_list.append(x)\n",
    "        f_vals.append(get_value(x))\n",
    "        x = gradient_descent_update(x=x, eta=eta)\n",
    "    x_list = np.array(x_list)\n",
    "\n",
    "\n",
    "    plot_error_vs_iteration(f_vals, plt_title='GD iterate loss for eta='+str(eta))\n",
    "\n",
    "    if n_dim==2:\n",
    "        plot_trajectory(x_list, plt_title='GD trajectory with eta='+str(eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 part 5:\n",
    "for stepsize_rule in ['AG', 'FR']:\n",
    "    f_vals=[]\n",
    "    x_list=[]\n",
    "    x=np.zeros((n_dim))+np.random.randn(n_dim)*0.1\n",
    "\n",
    "    f_vals = []\n",
    "    for t in range(20):\n",
    "\n",
    "        x_list.append(x)\n",
    "        f_vals.append(get_value(x))\n",
    "        if stepsize_rule=='AG':\n",
    "            x = gradient_descent_update_AG(x=x, eta=eta)\n",
    "        elif stepsize_rule=='FR':\n",
    "            x = gradient_descent_update_FR(x=x, eta=eta)\n",
    "            \n",
    "    x_list = np.array(x_list)\n",
    "\n",
    "\n",
    "    plot_error_vs_iteration(f_vals, plt_title='GD iterate loss for step size rule '+stepsize_rule)\n",
    "\n",
    "    if n_dim==2:\n",
    "        plot_trajectory(x_list, plt_title='GD trajectory with step size rule '+ stepsize_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 part 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "\n",
    "Assume you are given a value and gradient oracle to a quadratic function. Minimise this using only the oracle.\n",
    "\n",
    "1. Write the Conjugate gradient algorithm update rule.\n",
    "2. Run on f1 and generate error plot and trajectory plots. (Create a new cell for this.)\n",
    "3. On a given 100-dimensional convex quadratic plot f(x_t) vs t for both Gradient descent (with full relaxation) and CGD for 20 iterations.\n",
    "4. Explain your findings in a new markdown cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Question 2, part 1\n",
    "\n",
    "def CG_update(x_t, d_t):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x_t : 1 dimensional numpy array as input. Representing current iterate x_t\n",
    "    d_t: represents the current direction of movement. (d_t)\n",
    "\n",
    "    Returns:\n",
    "    x_next: numpy array with same shape as x_t, representing x_{t+1}\n",
    "    d_next: numpy array with same shape as d_t, representing d_{t+1}\n",
    "    \n",
    "    x_next corresponds to one Conjugate gradient descent step with step size given by full relaxation. \n",
    "    You may assume the function that you have oracle access to is a quadratic. But you don't know A or b. \n",
    "    You only have access to the oracles.\n",
    "    You need to figure out how to get the required quantities like r_t, v^T A v using zeroth and first order oracles.\n",
    "    \n",
    "    Notes:\n",
    "    You are allowed one get_gradient call in this function. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test function 1: Convex 2d- Quadratic\n",
    "n_dim = 2\n",
    "def get_value(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    f(x) : a scalar\n",
    "    \"\"\"\n",
    "    A=np.array([[5.,4],[4.,5.]])\n",
    "    b=np.array([7.,2.])\n",
    "    return 0.5*np.dot(x, np.dot(A,x)) - np.dot(b,x)\n",
    "\n",
    "def get_gradient(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    grad f(x) : numpy array with same 1-dimensional shape as input x\n",
    "    \"\"\"\n",
    "    A=np.array([[5.,4],[4.,5.]])\n",
    "    b=np.array([7.,2.])\n",
    "    return np.dot(A,x) - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2, part 2\n",
    "\n",
    "f_vals=[]\n",
    "x_list=[]\n",
    "\n",
    "x=np.zeros((n_dim))+np.random.randn(n_dim)*0.1\n",
    "d=-1*get_gradient(x)\n",
    "\n",
    "f_vals = []\n",
    "for t in range(20):\n",
    "    x_list.append(x)\n",
    "    f_vals.append(get_value(x))    \n",
    "    x, d = CG_update(x, d)\n",
    "x_list = np.array(x_list)\n",
    "\n",
    "\n",
    "plot_error_vs_iteration(f_vals, plt_title='GD iterate loss for eta='+str(eta))\n",
    "\n",
    "if n_dim==2:\n",
    "    plot_trajectory(x_list, plt_title='GD trajectory with eta='+str(eta))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test function 3: Convex 100d- Quadratic\n",
    "n_dim = 100\n",
    "def get_value(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    f(x) : a scalar\n",
    "    \"\"\"\n",
    "    A=np.diag(np.arange(1.,n_dim+1.,1.))\n",
    "    b=np.arange(1,n_dim + 1., 1.)\n",
    "    return 0.5*np.dot(x, np.dot(A,x)) - np.dot(b,x)\n",
    "\n",
    "def get_gradient(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: 1 dimensional numpy array as input.\n",
    "    \n",
    "    Returns:\n",
    "    grad f(x) : numpy array with same 1-dimensional shape as input x\n",
    "    \"\"\"\n",
    "    A=np.diag(np.arange(1.,n_dim+1.,1.))\n",
    "    b=np.arange(1,n_dim + 1., 1.)\n",
    "    return np.dot(A,x) - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2, part 3\n",
    "\n",
    "f_vals_GD=[]\n",
    "x=np.random.randn(n_dim)*0.1\n",
    "for t in range(20):\n",
    "    f_vals_GD.append(get_value(x))\n",
    "    x = gradient_descent_update_FR(x=x)\n",
    "\n",
    "f_vals_CGD=[]\n",
    "x=np.random.randn(n_dim)*0.1\n",
    "d=-1*get_gradient(x)\n",
    "for t in range(20):\n",
    "    f_vals_CGD.append(get_value(x))\n",
    "    x, d = CG_update(x,d)\n",
    "    \n",
    "plt.plot(range(20),f_vals_GD, 'b-', range(20), f_vals_CGD, 'r-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2, part 4:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
